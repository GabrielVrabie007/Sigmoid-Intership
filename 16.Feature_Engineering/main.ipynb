{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695418bf",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cd2e4",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d5c9fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6d29666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/HousingData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ee99bd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     486 non-null    float64\n",
      " 1   ZN       486 non-null    float64\n",
      " 2   INDUS    486 non-null    float64\n",
      " 3   CHAS     486 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      486 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    int64  \n",
      " 9   TAX      506 non-null    int64  \n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    486 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef769af1",
   "metadata": {},
   "source": [
    "# Boston Housing Dataset: Feature Descriptions\n",
    "\n",
    "---\n",
    "### Features\n",
    "\n",
    "-   `CRIM`: Per capita crime rate by town.\n",
    "-   `ZN`: Proportion of residential land zoned for lots over 25,000 sq. ft.\n",
    "-   `INDUS`: Proportion of non-retail business acres per town.\n",
    "-   `CHAS`: Charles River dummy variable (`1` if tract bounds river; `0` otherwise).\n",
    "-   `NOX`: Nitric oxide concentration (parts per 10 million).\n",
    "-   `RM`: Average number of rooms per dwelling.\n",
    "-   `AGE`: Proportion of owner-occupied units built prior to 1940.\n",
    "-   `DIS`: Weighted distances to five Boston employment centers.\n",
    "-   `RAD`: Index of accessibility to radial highways.\n",
    "-   `TAX`: Full-value property tax rate per $10,000.\n",
    "-   `PTRATIO`: Pupil-teacher ratio by town.\n",
    "-   `B`: `1000(Bk — 0.63)²`, where `Bk` is the proportion of people of African American descent by town.\n",
    "-   `LSTAT`: Percentage of lower status of the population.\n",
    "\n",
    "---\n",
    "\n",
    "### Target Variable\n",
    "\n",
    "> **`MEDV`**: Median value of owner-occupied homes in $1000s. This is typically the target variable for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5b397166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       20\n",
       "ZN         20\n",
       "INDUS      20\n",
       "CHAS       20\n",
       "NOX         0\n",
       "RM          0\n",
       "AGE        20\n",
       "DIS         0\n",
       "RAD         0\n",
       "TAX         0\n",
       "PTRATIO     0\n",
       "B           0\n",
       "LSTAT      20\n",
       "MEDV        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f06e97",
   "metadata": {},
   "source": [
    "## 1.Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56512ff5",
   "metadata": {},
   "source": [
    "### I highly recommend to use sklearn.impute instead of reparo because of some unexpected errors and the principle of work\n",
    "\n",
    "```python\n",
    "from reparo import KNNImputer\n",
    "\n",
    "columns_to_impute = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'AGE', 'LSTAT']\n",
    "imputer = KNNImputer(n_neighbors=6)\n",
    "\n",
    "df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])\n",
    "```\n",
    "\n",
    "\n",
    "❗ AttributeError: 'DataFrame' object has no attribute 'dtype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1521fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "columns_to_impute = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'AGE', 'LSTAT']\n",
    "imputer = KNNImputer(n_neighbors=6)\n",
    "\n",
    "df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78796b4",
   "metadata": {},
   "source": [
    "## 2.Handle Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedcd0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 26 outliers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "columns_to_check = df.columns\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "iso_forest.fit(df[columns_to_check])\n",
    "\n",
    "outlier_labels = iso_forest.predict(df[columns_to_check])\n",
    "\n",
    "df_clean = df[outlier_labels == 1]\n",
    "\n",
    "\n",
    "#Because is a small amount of outliers ,could be removed ,but this is not always the right approach\n",
    "print(f\"Removed {sum(outlier_labels == -1)} outliers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c856195",
   "metadata": {},
   "source": [
    "## 3. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef9a2b",
   "metadata": {},
   "source": [
    "#### If categorical data exists it should be encoded , if you want to use them later for something because ML is more adapted to numeric forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61ead0",
   "metadata": {},
   "source": [
    "## 4.Scalling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "31ced0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model without polynomial features\n",
      "Model Accuracy: 0.7929928866668681\n",
      "Mean Squared Error: 14.679095205055257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df_clean.drop(columns=['MEDV'])\n",
    "y = df_clean['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "accuracy_score = model.score(X_test, y_test)\n",
    "print(\"Trained Model without polynomial features\")\n",
    "print(f\"Model Accuracy: {accuracy_score}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f736c2b0",
   "metadata": {},
   "source": [
    "## 5.Polynomial Features == from existing features make new and add them in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a313c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 1.0, 'fit_intercept': True}\n",
      "Model Accuracy (R²): 0.8392578867685252\n",
      "Mean Squared Error: 11.787827276449052\n",
      "R^2 Score: 0.8392578867685252\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "poly_cols = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE',\n",
    "             'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "# Add new polynomial features to my dataset\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly_features = poly.fit_transform(df[poly_cols])\n",
    "feature_names = poly.get_feature_names_out(poly_cols)\n",
    "\n",
    "df_poly = pd.DataFrame(X_poly_features, columns=feature_names)\n",
    "\n",
    "df_with_poly = pd.concat([df.reset_index(drop=True), df_poly], axis=1)\n",
    "\n",
    "X = df_with_poly.drop(columns=['MEDV'])\n",
    "y = df_with_poly['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Use Ridge regression with hyperparameter tuning it is the same as linear regression but with regularization\n",
    "ridge = Ridge()\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "#Hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "accuracy_score = best_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Model Accuracy (R²): {accuracy_score}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252aa74",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "#### Feature engineering significantly improved model performance. By imputing missing values, removing outliers, and adding polynomial features, the model's R² score increased from 0.79 to 0.83, and the MSE dropped from 14.68 to 11.78.\n",
    "\n",
    "#### PolynomialFeatures captured non-linear relationships, and Ridge regularization prevented overfitting. This approach is like a boost for regression accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
